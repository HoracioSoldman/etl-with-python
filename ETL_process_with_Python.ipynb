{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ETL process with Python.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOzUMNnsRdWaqesB+nHFIEY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HoracioSoldman/simple-etl-with-python/blob/main/ETL_process_with_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A simple ETL process with Python"
      ],
      "metadata": {
        "id": "PjXgjwuSdeYV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we are going to implement a basic ETL process with Python, using the world happiness data for many countries.\n",
        "\n",
        "### **The data**\n",
        "We will employ two similar datasets:\n",
        "1.    A dataset from [World Happiness Report 2021](https://worldhappiness.report/ed/2021/) in```csv``` format.\n",
        "\n",
        "2.    The other dataset is originated from the [World Population Review](https://worldpopulationreview.com/country-rankings/happiest-countries-in-the-world) website, more precisely at the \"Happiest Countries in the World 2021\" section. We will download the one in ```json``` format.\n",
        "\n",
        "\n",
        "### **The transformation**\n",
        "\n",
        "In this notebook, we are interested in the **Happiness Score** of each country over the years in our datasets. The scores are labeled as ```Life Ladder``` in our first dataset and ```happiness2021```, ```happiness2020``` in the second one. The main process of the transformation includes:\n",
        "\n",
        "\n",
        "1.   Keeping only the the country name, the life ladder index and the corresponding year from the first dataset.\n",
        "\n",
        "2.   Extracting the similar values from the second dataset.\n",
        "\n",
        "3.   Combining both datasets \n",
        "\n",
        "4.   Removing duplicated entries\n",
        "(e.g If there are two rows with 'Afghanistan 2020', we only keep the first one.)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FkfZbUQRdmHD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {
        "id": "ON8C2FfRdZbp"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import pandas as pd\n",
        "import glob\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Global variables"
      ],
      "metadata": {
        "id": "TnI0hT6DA4Wp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_file= 'log.txt'\n",
        "\n",
        "output_file= 'output.csv'"
      ],
      "metadata": {
        "id": "yuURvp3rBAvL"
      },
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The ETL functions"
      ],
      "metadata": {
        "id": "g62H2M7tFBCQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract"
      ],
      "metadata": {
        "id": "CWqVETwhtaxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def downloader():\n",
        "  \n",
        "  # download the excel file from the World Happiness website\n",
        "  !wget https://happiness-report.s3.amazonaws.com/2021/DataPanelWHR2021C2.xls -O HR2008-21.xls\n",
        "\n",
        "  # download the json file originated from the World Population Review website\n",
        "  !wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1ATQG7rzhCbYHISjKPlPFUdPFr3l1AWeR' -O WHR2020-21.json\n"
      ],
      "metadata": {
        "id": "M6yxuN_wN_K3"
      },
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract json file\n",
        "def extractor():\n",
        "  dataframes= {}\n",
        "\n",
        "  # retrieve data from sources\n",
        "  downloader()\n",
        "\n",
        "  files= glob.glob('*.*')\n",
        "  \n",
        "  for f in files:\n",
        "    \n",
        "    if (f.endswith('.xls')):\n",
        "      df= pd.read_excel(f)\n",
        "      dataframes['main_df']= df\n",
        "    \n",
        "    elif(f.endswith('.json')):\n",
        "      df= pd.read_json(f)\n",
        "      dataframes['additional_df']= df\n",
        "  \n",
        "  return dataframes"
      ],
      "metadata": {
        "id": "5I4jasoV2Fue"
      },
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transform"
      ],
      "metadata": {
        "id": "HBGMYBvi81DH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_dfs(dfs):\n",
        "  df=pd.concat(dfs, axis=0, ignore_index=True)\n",
        "  return df"
      ],
      "metadata": {
        "id": "KufdoAyK-axZ"
      },
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer(dfs):\n",
        "  main_df= dfs['main_df']\n",
        "  additional_df= dfs['additional_df']\n",
        "\n",
        "  # for the main dataframe only keep relevant data from the first dataset\n",
        "  df0= main_df[['Country name', 'year', 'Life Ladder']]\n",
        "\n",
        "  # for the additional dataset, save the contents of df1 in a dictionary\n",
        "  df1_data= {'Country name': [], 'year': [], 'Life Ladder': []}\n",
        "\n",
        "  for index, row in additional_df.iterrows():\n",
        "    df1_data['Country name'].append(row['country'])\n",
        "    df1_data['year'].append(2020)\n",
        "    df1_data['Life Ladder'].append(row['happiness2020'])\n",
        "\n",
        "    df1_data['Country name'].append(row['country'])\n",
        "    df1_data['year'].append(2021)\n",
        "    df1_data['Life Ladder'].append(row['happiness2021'])\n",
        "\n",
        "  # convert the dictionary into a new dataframe df1\n",
        "  df1= pd.DataFrame(df1_data)\n",
        "\n",
        "  # combine the dataframes\n",
        "  df=combine_dfs([df0, df1])\n",
        "\n",
        "  # remove duplicated values on the combination of 'Country name' and 'year'\n",
        "  df.drop_duplicates(subset=['Country name', 'year'], keep='first', inplace=True)\n",
        "\n",
        "  # return the new dataframe\n",
        "  return df\n"
      ],
      "metadata": {
        "id": "KM3Un3bf8LQe"
      },
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load"
      ],
      "metadata": {
        "id": "NJZDE1ebmLlF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the sake of simplicity, let us just save the combined data into a ```happiness-report.csv``` file which will can be loaded into any data repository in the future."
      ],
      "metadata": {
        "id": "fK10FtE0mZqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loader(df):\n",
        "  df.to_csv(output_file)"
      ],
      "metadata": {
        "id": "NDxlmVyGmP-e"
      },
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Log"
      ],
      "metadata": {
        "id": "2kGw7YeZC1-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logger(text):\n",
        "  timestamp_format = '%H:%M:%S-%h-%d-%Y' #Hour-Minute-Second-MonthName-Day-Year\n",
        "  current_time = datetime.now()\n",
        "\n",
        "  timestamp = current_time.strftime('%d-%m-%Y %H:%M:%S:%f')\n",
        "  with open(log_file, \"a\") as f:\n",
        "      f.write(timestamp +'> '+ text +'\\n') "
      ],
      "metadata": {
        "id": "doQP0GLuC4r1"
      },
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run"
      ],
      "metadata": {
        "id": "nHqcZkZnF10h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is important to log all the different steps of the process by invoking the ```logger``` function."
      ],
      "metadata": {
        "id": "9GR_RFHOGFeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logger('ETL: Starting the ETL process.')"
      ],
      "metadata": {
        "id": "bqbRupXaF7bk"
      },
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger('EXTRACT: Starting the Extraction stage.')\n",
        "\n",
        "dataframes= extractor()\n",
        "\n",
        "logger('EXTRACT: Completed the Extraction stage.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQcFvNPEGj88",
        "outputId": "1322a1cd-9e28-45b2-b781-d4308d51129e"
      },
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-22 15:10:12--  https://happiness-report.s3.amazonaws.com/2021/DataPanelWHR2021C2.xls\n",
            "Resolving happiness-report.s3.amazonaws.com (happiness-report.s3.amazonaws.com)... 52.216.176.187\n",
            "Connecting to happiness-report.s3.amazonaws.com (happiness-report.s3.amazonaws.com)|52.216.176.187|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 434688 (424K) [application/vnd.ms-excel]\n",
            "Saving to: ‘HR2008-21.xls’\n",
            "\n",
            "\rHR2008-21.xls         0%[                    ]       0  --.-KB/s               \rHR2008-21.xls       100%[===================>] 424.50K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-01-22 15:10:12 (18.9 MB/s) - ‘HR2008-21.xls’ saved [434688/434688]\n",
            "\n",
            "--2022-01-22 15:10:12--  https://docs.google.com/uc?export=download&id=1ATQG7rzhCbYHISjKPlPFUdPFr3l1AWeR\n",
            "Resolving docs.google.com (docs.google.com)... 142.250.65.78, 2607:f8b0:4004:836::200e\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.65.78|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-14-2g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/9c437aer5k4gm9t3d9scnntop80p049s/1642864200000/00305885236840532660/*/1ATQG7rzhCbYHISjKPlPFUdPFr3l1AWeR?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-01-22 15:10:12--  https://doc-14-2g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/9c437aer5k4gm9t3d9scnntop80p049s/1642864200000/00305885236840532660/*/1ATQG7rzhCbYHISjKPlPFUdPFr3l1AWeR?e=download\n",
            "Resolving doc-14-2g-docs.googleusercontent.com (doc-14-2g-docs.googleusercontent.com)... 172.217.12.225, 2607:f8b0:4004:807::2001\n",
            "Connecting to doc-14-2g-docs.googleusercontent.com (doc-14-2g-docs.googleusercontent.com)|172.217.12.225|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20703 (20K) [application/json]\n",
            "Saving to: ‘WHR2020-21.json’\n",
            "\n",
            "WHR2020-21.json     100%[===================>]  20.22K  --.-KB/s    in 0.005s  \n",
            "\n",
            "2022-01-22 15:10:12 (4.04 MB/s) - ‘WHR2020-21.json’ saved [20703/20703]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logger('TRANSFORM: Starting the Transformation stage.')\n",
        "\n",
        "dataframe= transformer(dataframes)\n",
        "\n",
        "logger('TRANSFORM: Completed the Transformation stage.')"
      ],
      "metadata": {
        "id": "sR-e0O_BHWKU"
      },
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger('LOAD: Starting the Loading stage.')\n",
        "\n",
        "loader(dataframe)\n",
        "\n",
        "logger('LOAD: Completed the Loading stage.')"
      ],
      "metadata": {
        "id": "tU1aP0-mH0CD"
      },
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger('ETL: Completed the ETL process.')"
      ],
      "metadata": {
        "id": "r_LXx6maOir3"
      },
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read the log file\n",
        "with open(log_file, 'r') as f:\n",
        "    for line in f:\n",
        "        print(line) \n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKxEIw4kPj3V",
        "outputId": "1a22f06d-6739-457e-ebc3-e7eecabfdd52"
      },
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22-01-2022 15:10:12:247020> ETL: Starting the ETL process.\n",
            "\n",
            "22-01-2022 15:10:12:258556> EXTRACT: Starting the Extraction stage.\n",
            "\n",
            "22-01-2022 15:10:12:993728> EXTRACT: Completed the Extraction stage.\n",
            "\n",
            "22-01-2022 15:10:13:002649> TRANSFORM: Starting the Transformation stage.\n",
            "\n",
            "22-01-2022 15:10:13:030542> TRANSFORM: Completed the Transformation stage.\n",
            "\n",
            "22-01-2022 15:10:13:039077> LOAD: Starting the Loading stage.\n",
            "\n",
            "22-01-2022 15:10:13:054718> LOAD: Completed the Loading stage.\n",
            "\n",
            "22-01-2022 15:10:13:062205> ETL: Completed the ETL process.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}